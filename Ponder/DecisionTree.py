# -*- coding: utf-8 -*-
"""DecisionTree.py

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1RVzFhyge8Yv9FHKdJ2-Kg0_XzOSfalc2
"""

from sklearn import tree
from sklearn.tree import export_text
import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split
import pandas as pd

from sklearn.metrics import mean_squared_error
from sklearn.metrics import mean_absolute_error

# To predict the political party according to the issues they supported
vote_url = "https://archive.ics.uci.edu/ml/machine-learning-databases/voting-records/house-votes-84.data"
columns = ["Class Name", "handicapped-infants", "water-project-cost-sharing", "adoption-of-the-budget-resolution", "physician-fee-freeze", "el-salvador-aid",
           "religious-groups-in-schools", "anti-satellite-test-ban", "aid-to-nicaraguan-contras", "mx-missile", "immigration", "synfuels-corporation-cutback",
           "education-spending", "superfund-right-to-sue", "crime", "duty-free-exports", "export-administration-act-south-africa"]
df = pd.read_csv(vote_url, names=columns, na_values=["?"])
# As NaN is also an option not a mistake
df = df.fillna(2)
df = df.replace(to_replace = 'y', value = 1) 
df = df.replace(to_replace = 'n', value = 0) 


targets, features, y= np.hsplit(df,np.array([1,17]))

train_data, test_data, train_targets, test_targets = train_test_split(features, targets, test_size=.3)

clf = tree.DecisionTreeClassifier()
clf = clf.fit(train_data, train_targets)

predictions = clf.predict(test_data)
result = np.equal(test_targets.to_numpy().flatten(), predictions.flatten())
accuracy = np.count_nonzero(result == 1) / len(predictions)

feature_name = ["handicapped-infants", "water-project-cost-sharing", "adoption-of-the-budget-resolution", "physician-fee-freeze", "el-salvador-aid",
           "religious-groups-in-schools", "anti-satellite-test-ban", "aid-to-nicaraguan-contras", "mx-missile", "immigration", "synfuels-corporation-cutback",
           "education-spending", "superfund-right-to-sue", "crime", "duty-free-exports", "export-administration-act-south-africa"]

r = export_text(clf, feature_names= feature_name)

print("Accuracy: ", accuracy*100, "%")
print(tree.plot_tree(clf)) 
print(r)

# putting the max_depth as 2 has slightly increase the accuracy
clf = tree.DecisionTreeClassifier(random_state=0, max_depth=2)
clf = clf.fit(train_data, train_targets)

predictions = clf.predict(test_data)
result = np.equal(test_targets.to_numpy().flatten(), predictions.flatten())
accuracy = np.count_nonzero(result == 1) / len(predictions)

feature_name = ["handicapped-infants", "water-project-cost-sharing", "adoption-of-the-budget-resolution", "physician-fee-freeze", "el-salvador-aid",
           "religious-groups-in-schools", "anti-satellite-test-ban", "aid-to-nicaraguan-contras", "mx-missile", "immigration", "synfuels-corporation-cutback",
           "education-spending", "superfund-right-to-sue", "crime", "duty-free-exports", "export-administration-act-south-africa"]

r = export_text(clf, feature_names= feature_name)

print("Accuracy: ", accuracy*100, "%")
print(tree.plot_tree(clf)) 
print(r)

# Credit Approval one-hot and dropna
crx_url = "https://archive.ics.uci.edu/ml/machine-learning-databases/credit-screening/crx.data"
columns = ["A1","A2","A3","A4","A5","A6","A7","A8","A9","A10","A11","A12","A13","A14","A15","A16"]
df = pd.read_csv(crx_url, names=columns, na_values=["?"])
# because all the features' name are hidden. It is pretty hard to determine what to do with the missing data.
# what I am going to do is to delete all row with missing data
df.dropna(axis=0, how='any', thresh=None, subset=None, inplace=True)

features, y, targets= np.hsplit(df,np.array([15,15]))

features = features.replace(to_replace = 't', value = 1) 
features = features.replace(to_replace = 'f', value = 0) 
features = features.replace(to_replace = 'b', value = 1) 
features = features.replace(to_replace = 'a', value = 0) 

features = pd.get_dummies(features, columns=["A4"])
features = pd.get_dummies(features, columns=["A5"])
features = pd.get_dummies(features, columns=["A6"])
features = pd.get_dummies(features, columns=["A7"])
features = pd.get_dummies(features, columns=["A13"])


targets = targets.replace(to_replace = '+', value = 1) 
targets = targets.replace(to_replace = '-', value = 0) 

train_data, test_data, train_targets, test_targets = train_test_split(features, targets, test_size=.3)



clf = tree.DecisionTreeRegressor(random_state=0, max_depth=3)
clf = clf.fit(train_data, train_targets)

predictions = clf.predict(test_data)
result = np.equal(test_targets.to_numpy().flatten(), predictions.flatten())
accuracy = np.count_nonzero(result == 1) / len(predictions)

feature_name = ["A1",	"A2",	"A3"	,"A8"	,"A9",	"A10",	"A11",	"A12",	"A14",	"A15",	"A4_l",	"A4_u",	"A4_y"	,"A5_g"	,"A5_gg"	,"A5_p"	,"A6_aa",	"A6_c"	,"A6_cc",	"A6_d",	"A6_e"	,"A6_ff",	"A6_i",	"A6_j",	"A6_k",	"A6_m",	"A6_q",	"A6_r",	"A6_w",	"A6_x",	"A7_bb",	"A7_dd"	,"A7_ff"	,"A7_h"	,"A7_j"	,"A7_n",	"A7_o",	"A7_v"	,"A7_z",	"A13_g"	,"A13_p"	,"A13_s"]

r = export_text(clf, feature_names= feature_name)

print ('MSE = ' , mean_squared_error(test_targets, predictions))
print ('MAE = ', mean_absolute_error(test_targets, predictions))
print(tree.plot_tree(clf)) 
print(r)

crx_url = "https://archive.ics.uci.edu/ml/machine-learning-databases/credit-screening/crx.data"
columns = ["A1","A2","A3","A4","A5","A6","A7","A8","A9","A10","A11","A12","A13","A14","A15","A16"]
df = pd.read_csv(crx_url, names=columns, na_values=["?"])
# because all the features' name are hidden. It is pretty hard to determine what to do with the missing data.
# what I am going to do is to delete all row with missing data
df.dropna(axis=0, how='any', thresh=None, subset=None, inplace=True)

features, y, targets= np.hsplit(df,np.array([15,15]))

features = features.replace(to_replace = 't', value = 1) 
features = features.replace(to_replace = 'f', value = 0) 
features = features.replace(to_replace = 'b', value = 1) 
features = features.replace(to_replace = 'a', value = 0) 

features["A4"] = features["A4"].astype('category')
features["A4"] = features["A4"].cat.codes
features["A5"] = features["A5"].astype('category')
features["A5"] = features["A5"].cat.codes
features["A6"] = features["A6"].astype('category')
features["A6"] = features["A6"].cat.codes
features["A7"] = features["A7"].astype('category')
features["A7"] = features["A7"].cat.codes
features["A13"] = features["A13"].astype('category')
features["A13"] = features["A13"].cat.codes


targets = targets.replace(to_replace = '+', value = 1) 
targets = targets.replace(to_replace = '-', value = 0) 

train_data, test_data, train_targets, test_targets = train_test_split(features, targets, test_size=.3)



clf = tree.DecisionTreeRegressor(random_state=0, max_depth=3)
clf = clf.fit(train_data, train_targets)

predictions = clf.predict(test_data)
result = np.equal(test_targets.to_numpy().flatten(), predictions.flatten())
accuracy = np.count_nonzero(result == 1) / len(predictions)

feature_name = ["A1","A2","A3","A4","A5","A6","A7","A8","A9","A10","A11","A12","A13","A14","A15"]

r = export_text(clf, feature_names= feature_name)

print ('MSE = ' , mean_squared_error(test_targets, predictions))
print ('MAE = ', mean_absolute_error(test_targets, predictions))
print(tree.plot_tree(clf)) 
print(r)

krkopt_url = "https://archive.ics.uci.edu/ml/machine-learning-databases/chess/king-rook-vs-king/krkopt.data"
columns = ["White King file", "White King rank", "White Rook file", "White Rook rank", "Black King file", "Black King rank", "depth-of-win for White"]
df = pd.read_csv(krkopt_url, names=columns, na_values=["?"])


features, y, targets= np.hsplit(df,np.array([6,6]))

#Because file represent the column of location, it is better to put it as label.
features = features.replace(to_replace = 'a', value = 1) 
features = features.replace(to_replace = 'b', value = 2) 
features = features.replace(to_replace = 'c', value = 3) 
features = features.replace(to_replace = 'd', value = 4) 
features = features.replace(to_replace = 'e', value = 5) 
features = features.replace(to_replace = 'f', value = 6) 
features = features.replace(to_replace = 'g', value = 7)
features = features.replace(to_replace = 'h', value = 8)


train_data, test_data, train_targets, test_targets = train_test_split(features, targets, test_size=.3)

clf = tree.DecisionTreeClassifier(random_state=0, max_depth=20)
clf = clf.fit(train_data, train_targets)

predictions = clf.predict(test_data)
result = np.equal(test_targets.to_numpy().flatten(), predictions.flatten())
accuracy = np.count_nonzero(result == 1) / len(predictions)

feature_name = ["White King file", "White King rank", "White Rook file", "White Rook rank", "Black King file", "Black King rank"]

r = export_text(clf, feature_names= feature_name)

print("Accuracy: ", accuracy*100, "%")
print(tree.plot_tree(clf)) 
print(r)