# -*- coding: utf-8 -*-
"""Neural Network.py

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1gMr8nKDLx-Gr9cIFliv4j1v3qaBk9Y4c
"""

import keras

from keras.models import Sequential

from keras.layers import Dense, Activation, Dropout

from keras.layers.convolutional import Conv1D

from keras.preprocessing.text import Tokenizer

from keras.preprocessing.sequence import pad_sequences

from sklearn.model_selection import train_test_split

import pandas as pd

import numpy as np

import spacy

import io

from google.colab import files

nlp=spacy.load("en")

#load the dataset

columns = ["polarity", "tweet ID", "date", "query", "username", "text"]

df = pd.read_csv("/content/drive/My Drive/Dataset/TweetPolarity.csv", names=columns, na_values=["?"], encoding= "latin-1")

df = df.drop(columns=["tweet ID", "date", "query", "username"])

import re
from sklearn.feature_extraction.text import CountVectorizer
from keras.preprocessing.text import Tokenizer
from keras.preprocessing.sequence import pad_sequences
# Higher the polarity, more positive the sentance is

targets,y, features = np.hsplit(df,np.array([1,1]))

#encoding the sentances into vectors
features = features.apply(lambda x: x.str.lower())
features= features.replace(to_replace=r'[^a-zA-z0-9\s]', value='', regex=True) 

max_features = 690960
tokenizer = Tokenizer(num_words=max_features, split=' ')
tokenizer.fit_on_texts(df['text'].values)
features = tokenizer.texts_to_sequences(df['text'].values)
features = pad_sequences(features, maxlen=100)

word_index = tokenizer.word_index   # one hot coding
vocab_size = len(tokenizer.word_index) + 1
print('Found %s unique tokens.' % len(word_index))

targets = pd.get_dummies(df.polarity)
ax = targets.plot.hist(bins=12, alpha=0.5)

print(targets)

# split the data into test and train

train_data, test_data, train_targets, test_targets = train_test_split(features, targets, test_size=.02, random_state=42 )

from keras.layers import Embedding

embeddings_index = dict()
f = open('/content/drive/My Drive/Dataset/GloVe/glove.6B.100d.txt')
for line in f:
	values = line.split()
	word = values[0]
	coefs = np.asarray(values[1:], dtype='float32')
	embeddings_index[word] = coefs
f.close()
print('Loaded %s word vectors.' % len(embeddings_index))
# create a weight matrix for words in training docs
embedding_matrix = np.zeros((vocab_size, 100))
for word, i in tokenizer.word_index.items():
	embedding_vector = embeddings_index.get(word)
	if embedding_vector is not None:
		embedding_matrix[i] = embedding_vector
print(embedding_matrix.shape)

from __future__ import print_function

from keras.preprocessing import sequence
from keras.models import Sequential
from keras.layers import Dense, Embedding
from keras.layers import LSTM
from keras.datasets import imdb

max_features = 20000
# cut texts after this number of words (among top max_features most common words)
maxlen = 100
batch_size = 10

print('Loading data...')
print(len(train_data), 'train sequences')
print(len(test_data), 'test sequences')

print('Pad sequences (samples x time)')
train_data = sequence.pad_sequences(train_data, maxlen=maxlen)
test_data = sequence.pad_sequences(test_data, maxlen=maxlen)
print('train_data shape:', train_data.shape)
print('test_data shape:', test_data.shape)

import tensorflow as tf

print('Build model...')
model = Sequential()
model.add(Embedding(vocab_size, 100, weights=[embedding_matrix], input_length=100, trainable=True))
model.add(LSTM(128, dropout=0.2, recurrent_dropout=0.2))

model.add(Dense(2, activation='softmax'))


dot_img_file = '/tmp/model_1.png'
tf.keras.utils.plot_model(model, to_file=dot_img_file, show_shapes=True)

# try using different optimizers and different optimizer configs
model.compile(loss='binary_crossentropy', 
              optimizer='adam', 
              metrics=['accuracy'])

print('Train...')
model.fit(train_data, train_targets,
          batch_size=batch_size,
          epochs=150,
          validation_data=(test_data, test_targets))
score, mse = model.evaluate(test_data, test_targets,
                            batch_size=batch_size)
print('Test score:', score)
print('Test mse:', mse)